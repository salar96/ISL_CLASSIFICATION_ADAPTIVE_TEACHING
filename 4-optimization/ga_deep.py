# -*- coding: utf-8 -*-
"""GA_Deep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PUJNVzqQu4UBJWTmpsUDbTwNhWCMitSr
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import numpy as np
import random
from random import choices
from keras.models import Sequential
from keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D
from matplotlib import pyplot as plt
from matplotlib.pyplot import figure
from scipy.io import loadmat
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.optimizers import SGD
from keras.callbacks import EarlyStopping,History
import time
import dill

from google.colab import drive
drive.mount('/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /gdrive/My Drive/

x = loadmat(r"data10.mat")
xx=x['out']
data=xx;
target=np.zeros(xx.shape[0])
for i in range(0,xx.shape[0]):
    target[i]=xx[i,0,60]
x_trn=data[:,:,:-1]
y_trn=target    
x_trn,x_vld,y_trn,y_vld=train_test_split(x_trn,y_trn,test_size=0.1,shuffle=True)
x_trn,x_tst,y_trn,y_tst=train_test_split(x_trn,y_trn,test_size=0.22222222,shuffle=True)
x_trn=x_trn.reshape(x_trn.shape[0],24,60,1)
x_vld=x_vld.reshape(x_vld.shape[0],24,60,1)
x_tst=x_tst.reshape(x_tst.shape[0],24,60,1)
y_trn=to_categorical(y_trn)
y_vld=to_categorical(y_vld)
y_tst=to_categorical(y_tst)

x_trn.shape[0]

N_ipop=32;
myu=0.1;
par_num=4;
max_iter=25;
repeat_num=1;
epoch_num=15
epoch_const=2.5
k1_min=10
k1_max=150;
k2_min=10;
k2_max=150;
f1_min=1;
f1_max=10;
f2_min=1;
f2_max=10;

est_time=(max_iter*N_ipop/2*epoch_num*epoch_const+N_ipop*epoch_num*epoch_const)/60
print(np.round(est_time))

def give_id(x):
    b=np.zeros(x.shape[0])
    for i in range (0,x.shape[0]):
        for j in range(0,x.shape[1]):
            if np.round(x[i,j])==1:
                b[i]=j;       
    return b

def un_norm(x):
    x[0]=int(x[0]*(k1_max-k1_min)+k1_min)
    x[1]=int(x[1]*(k2_max-k2_min)+k2_min)
    x[2]=int(x[2]*(f1_max-f1_min)+f1_min)
    x[3]=int(x[3]*(f2_max-f2_min)+f2_min)
    return x 
  
  
def return_model(x):
    k1=x[0]
    k2=x[1]
    f1=x[2]
    f2=x[3]
    model=Sequential();
    model.add(Conv2D(k1,kernel_size=f1,activation='relu',padding='same',input_shape=(24,60,1)))
    model.add(MaxPooling2D((3,5),padding='same'))
    model.add(Conv2D(k2,kernel_size=f2,activation='relu',padding='same'))
    model.add(MaxPooling2D((3,5),padding='same'))
    model.add(Dropout(0.01))
    model.add(Flatten())
    model.add(Dense(15,activation='softmax'))
    opt = SGD(lr=10e-5)
    model.compile(optimizer='adam',loss='categorical_crossentropy' , metrics=['accuracy'])
    history=History()
    cback=EarlyStopping(monitor='val_loss',
                                  min_delta=0,
                                  patience=5,
                                  verbose=0, mode='auto',restore_best_weights=True)
    h=model.fit(x_trn,y_trn,validation_data=(x_vld,y_vld),epochs=epoch_num,shuffle=False,callbacks = [cback,history])
    final_acc=100-(np.count_nonzero(give_id(np.round(model.predict(x_tst)))-give_id(y_tst))/y_tst.shape[0]*100)
    #criteria=-((np.max(history.history['val_accuracy'])*100+2*final_acc)/3+(-model.count_params())/1e4)
    #criteria=-np.max(history.history['val_accuracy'])
    #criteria=-model.count_params()
    criteria=-((1*np.mean(history.history['accuracy'])*100+2*np.mean(history.history['val_accuracy'])*100+3*final_acc)/6*1000+(1e5-model.count_params())/100)
    #this criteria is good because we mean all accuracies. a 98.5 accuracy with 200000 params is equal to a 99 accuracy with 700000 params.
    return [criteria,final_acc,model]
def mate(x,y):
    u=x.copy()
    v=y.copy()
    r=int(np.random.rand()*(x.shape[1]))
    
    a=u[0,r]
    b=v[0,r]
    u[0,r]=b
    v[0,r]=a
    uu=u.copy()
    vv=v.copy()
    beta=np.random.rand()
    #print('changed the {}th index with beta: {}'.format(r,beta))
    uu=(1-beta)*u+beta*v
    vv=beta*u+(1-beta)*v
    return uu,vv  

def isin(lis,arr):
  for i in range(0,len(lis)):
    if np.allclose(lis[i],arr) :
      return i
  return -1

param_list=[]
eval_list=[]
acc_list=[]
dummy=Sequential()
final_acc=0
acc=0

mean_cost=np.zeros(max_iter)
min_cost=np.zeros(max_iter)
for main_count in range(0,repeat_num):   
    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
    print("Here we gooooo!")
    ipop=np.random.rand(N_ipop,par_num)
    ipop_eval=np.zeros(N_ipop)
    print("Now we evaluate initial population:")
    for i in range (0,N_ipop):
        t=time.clock()
        ipop_eval[i],acc,dummy=return_model(un_norm(ipop[i,:].tolist()))
        print("number ",i+1,"of",N_ipop," is done! Accuracy is: ",acc,'cost is: ', ipop_eval[i],"with seq:",un_norm(ipop[i,:].tolist())," time(s):",time.clock()-t)
        param_list.append(ipop[i,:])
        eval_list.append(ipop_eval[i])
        acc_list.append(acc)
# no problem until here ()()()()()()()()()()

    I=np.argsort(ipop_eval)
    #print('I is ',I)
    npop=np.random.rand(ipop.shape[0],ipop.shape[1])
    for n in range (0,N_ipop):
        k=I[n]
        npop[n,:]=ipop[k,:]
        
    npop=npop[:int(N_ipop/2)]
    N=int(npop.shape[0]/2)
    pop_good=npop[:N]
    
    final_acc=np.max(acc_list)
    for coooo in range(1,max_iter+1):
        print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
        print("initial population is done, were going for the next ones!\n")
        print("repeat:",main_count,"iter: ",coooo,'of:',max_iter)
        p=np.array(range(0,npop.shape[0]))
        weights=np.zeros(npop.shape[0])
        for i in range(1,npop.shape[0]+1):
            weights[i-1]=(npop.shape[0]-i+1)/sum(p)
            #weights[i-1]=weights[i-1]+1/(N*i)*0.01
            #if i<10:
             #   weights[i-1]=weights[i-1]*4;
        pop_child=pop_good.copy()
        for i in range(0,int(pop_good.shape[0]),2):
            r1=choices(p,weights)   
            r2=choices(p,weights)
            while r1==r2:
              r2=choices(p,weights) 
            #print(r1,r2)
            pop_child[i,:],pop_child[i+1,:]=mate(npop[r1,:].reshape(1,par_num),npop[r2,:].reshape(1,par_num))
            #print('parents are: ',npop[r1,:].reshape(1,par_num),npop[r2,:].reshape(1,par_num))
            #print('childs are:' ,pop_child[i,:],pop_child[i+1,:] )
        # no problem until here()(((((())))))
        new_pop=np.concatenate((pop_good,pop_child))
        myu_num=int(np.round(myu*new_pop.shape[0]*new_pop.shape[1]))
        if myu_num<1:
            myu_num=1
        for i in range(1,myu_num+1):
            r1=int(np.random.rand()*(new_pop.shape[0]-1))+1
            while r1==0:
                r1=int(np.random.rand()*(new_pop.shape[0]-1))+1  
            r2=int(np.random.rand()*(new_pop.shape[1]))
            new_pop[r1,r2]=np.random.rand()
            #print('gene [{} {}] mutated!'.format(r1,r2))
        print("now we evaluate new population:")           
        #no problem until here((((((((()))))))))
        eeval=np.zeros(new_pop.shape[0])
        for i in range (0,new_pop.shape[0]):
            t=time.clock()
            iden=isin(param_list,new_pop[i,:])
            if iden != -1:
              eeval[i]=eval_list[iden]
              #print('WE HAVE SEEN IT BEFORE!')
            else:
              eeval[i],acc,dummy=return_model(un_norm(new_pop[i,:].tolist()))
              param_list.append(new_pop[i,:])
              eval_list.append(eeval[i])
              if acc>np.max(acc_list):
                final_acc=acc
              
              acc_list.append(acc)

              print("chromosome number ",i+1,"of",new_pop.shape[0]," is done! Accuracy is: ",acc,'cost is: ',eeval[i],"with seq:",un_norm(new_pop[i,:].tolist())," time(s):",time.clock()-t)

        mean_cost[coooo-1]=np.mean(eeval)
        min_cost[coooo-1]=np.min(eeval)
        I=np.argsort(eeval)
        npop=np.random.rand(new_pop.shape[0],new_pop.shape[1])
        for n in range (0,new_pop.shape[0]):
            k=I[n]
            npop[n,:]=new_pop[k,:]
        #new_pop=npop.copy()
        pop_good=npop[:N]

print("process complete!")
final=pop_good[0,:]                  
print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")                   
print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")                   
print("final params are: " , un_norm(final.tolist()))
print("final accuracy is: ",final_acc)
print("and the final model is:")
figure(num=None, figsize=(6, 10), dpi=80, facecolor='w', edgecolor='k')       
plt.subplot(211)
plt.plot(-mean_cost,linewidth=2,color='r')
plt.ylabel("Mean Fitness")
plt.xlabel("Iteration")
ptit="Mean Fitness diagram"
plt.title(ptit)
plt.grid()
plt.subplot(212)
plt.plot(-min_cost[:],linewidth=2,color='b')
plt.ylabel("Max Fitness")
plt.xlabel("Iteration")
ptit="Max Fitness diagram"
plt.title(ptit)
plt.grid()
plt.show()                           #pip install dill --user
filename = 'globalsave.pkl'
dill.dump_session(filename)

print(un_norm(pop_good[1,:]))

print("process complete!")
final=pop_good[0,:]                  
print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")                   
print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")                   
print("final params are: " , un_norm(final.tolist()))
print("final accuracy is: ",final_acc)
print("and the final model is:")
figure(num=None, figsize=(6, 10), dpi=80, facecolor='w', edgecolor='k')       
plt.subplot(211)
plt.plot(-mean_cost,linewidth=2,color='r')
plt.ylabel("Mean cost")
plt.xlabel("Iteration")
ptit="Mean cost diagram"
plt.title(ptit)
plt.grid()
plt.subplot(212)
plt.plot(-min_cost[:],linewidth=2,color='b')
plt.ylabel("Min cost")
plt.xlabel("Iteration")
ptit="Min cost diagram"
plt.title(ptit)
plt.grid()
plt.show()

isin(param_list,ipop[2,:])
#param_list[0]-ipop[0,:]
min_cost[-1]